{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Methods 1\n",
    "### [Gerard Gorman](http://www.imperial.ac.uk/people/g.gorman), [Matthew Piggott](http://www.imperial.ac.uk/people/m.d.piggott), [Nicolas Barral](http://www.imperial.ac.uk/people/n.barral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7: Numerical Linear Algebra III\n",
    "\n",
    "## Learning objectives:\n",
    "\n",
    "* Ill-conditioned matrices (matrix norms and condition number)\n",
    "* Direct vs iterative/indirect methods\n",
    "* Example iterative algorithm: the Jacobi and Gauss-Seidel methods\n",
    "* Sparse matrices and a pointer to more advanced algorithms (supplementary readings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ill-conditioned matrices\n",
    "\n",
    "The conditioning (or lack of, i.e. the ill-conditioning) of matrices we are trying to invert (to obtain the inverse, or to find the solution to a linear matrix system) is incredibly important for the success of any algorithm.\n",
    "\n",
    "When we started talking about matrices we noted that as long as the matrix is non-singular, i.e. $\\det(A)\\ne 0$, then an inverse exists, and a linear system with that $A$ has a unique solution.\n",
    "\n",
    "But what happens when we consider a matrix that is nearly singular, i.e. $\\det(A)$ is very small?\n",
    "\n",
    "Well smallness is a relative term and so we need to ask the question of how large or small $\\det(A)$ is compared to something.\n",
    "\n",
    "That something is the *norm* of the matrix.\n",
    "\n",
    "#### Vector norms\n",
    "\n",
    "Just as for vectors $\\pmb{v}$ (assumed a $n\\times 1$ column vector) where we have multiple possible norms to help us decide quantify the magnitude of a vector:\n",
    "\n",
    "\\begin{align}\n",
    "\\|\\pmb{v}\\|_2 & = \\sqrt{v_1^2 + v_2^2 + \\ldots + v_n^2} = \\left(\\sum_{i=1}^n v_i^2 \\right)^{1/2}, &\\quad{\\textrm{the two-norm or Euclidean norm}}\\\\\n",
    "\\|\\pmb{v}\\|_1  & = |v_1| + |v_2| + \\ldots + |v_n| = \\sum_{i=1}^n |v_i|, &\\quad{\\textrm{the one-norm or taxi-cab norm}}\\\\\n",
    "\\|\\pmb{v}\\|_{\\infty}  &= \\max\\{|v_1|,|v_2|, \\ldots, |v_n| = \\max_{i=1}^n |v_i|, &\\quad{\\textrm{the max-norm or infinity norm}}\n",
    "\\end{align}\n",
    "\n",
    "#### Matrix norms\n",
    "\n",
    "We can define measures of the size of matrices, e.g. for $A$ which for complete generality we will assume is of shape $m\\times n$:\n",
    "\n",
    "\\begin{align}\n",
    "\\|A\\|_F & = \\left(\\sum_{i=1}^m \\sum_{j=1}^n A_{ij}^2 \\right)^{1/2}, &\\quad{\\textrm{the matrix Euclidean or Frobenius norm}}\\\\\n",
    "\\|A\\|_{\\infty} & = \\max_{i=1}^m \\sum_{j=1}^n|A_{i,j}|, &\\quad{\\textrm{the maximum absolute row-sum norm}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Note that while these norms give different results (in both the vector and matrix cases), they are consistent or equivalent in that they are always within a constant factor of one another (a result that is true for finite-dimensional or discrete problems as here). This means we don't really need to worry too much about which norm we're using.\n",
    "\n",
    "Let's evaluate some examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10.   2.   1.]\n",
      " [  6.   5.   4.]\n",
      " [  1.   4.   7.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from scipy import linalg\n",
    "A=numpy.array([[10., 2., 1.],[6., 5., 4.],[1., 4., 7.]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.748015748\n"
     ]
    }
   ],
   "source": [
    "print(linalg.norm(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.748015748\n"
     ]
    }
   ],
   "source": [
    "# the Frobenius norm - the default\n",
    "print(linalg.norm(A,'fro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0\n"
     ]
    }
   ],
   "source": [
    "# the maximum absolute row-sum\n",
    "print(linalg.norm(A,numpy.inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.0\n"
     ]
    }
   ],
   "source": [
    "# the maximum absolute column-sum\n",
    "print(linalg.norm(A,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.7930910986\n"
     ]
    }
   ],
   "source": [
    "# the two-norm - note not the same as the Frobenius norm - also termed the spectral norm\n",
    "print(linalg.norm(A,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.7930910986\n"
     ]
    }
   ],
   "source": [
    "print(numpy.sqrt(numpy.real((numpy.max(linalg.eigvals(numpy.dot(A.T,A)))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Exercise 7.1: matrix norms</span>\n",
    "\n",
    "Write some code to explicitly compute the two matrix norms defined mathematically above (i.e. the Frobenius and the maximum absolute row-sum norms) and compare against the values found above using in-built scipy functions.\n",
    "\n",
    "Also, based on the above code and comments, what is the mathematical definition of the 1-norm and the 2-norm?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix conditioning\n",
    "\n",
    "The (ill-)conditioning of a matrix is measured with the matrix condition number:\n",
    "\n",
    "$$\\textrm{cond}(A) = \\|A\\|\\|A^{-1}\\|$$\n",
    "\n",
    "If this is close to one then $A$ is termed well-conditioned; the value increases with the degree of ill-conditioning, reaching infinity for a singular matrix.\n",
    "\n",
    "Let's evaluate the condition number for the matrix above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10.   2.   1.]\n",
      " [  6.   5.   4.]\n",
      " [  1.   4.   7.]]\n",
      "10.7133718813\n",
      "10.7133718813\n",
      "12.4636165619\n",
      "12.4636165619\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from scipy import linalg\n",
    "A=numpy.array([[10., 2., 1.],[6., 5., 4.],[1., 4., 7.]])\n",
    "print(A)\n",
    "print(numpy.linalg.cond(A))  # let's use the in-built condition number function\n",
    "print(linalg.norm(A,2)*linalg.norm(linalg.inv(A),2))  # so the default condition number uses the matrix two-norm\n",
    "print(numpy.linalg.cond(A,'fro'))\n",
    "print(linalg.norm(A,'fro')*linalg.norm(linalg.inv(A),'fro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition number is expensive to compute, and so in practice the relative size of the determinant of the matrix can be gauged based on the magnitude of the entries of the matrix.\n",
    "\n",
    "#### Example\n",
    "\n",
    "We know that a singular matrix does not result in a unique solution to its corresponding linear matrix system. But what are the consequences of near-singularity (ill-conditioning)?\n",
    "\n",
    "Consider the following example\n",
    "\n",
    "\n",
    "$$\n",
    "\\left(\n",
    "  \\begin{array}{cc}\n",
    "    2 & 1 \\\\\n",
    "    2 & 1 + \\epsilon  \\\\\n",
    "  \\end{array}\n",
    "\\right)\\left(\n",
    "  \\begin{array}{c}\n",
    "    x \\\\\n",
    "    y \\\\\n",
    "  \\end{array}\n",
    "\\right) = \\left(\n",
    "  \\begin{array}{c}\n",
    "    3 \\\\\n",
    "    0 \\\\\n",
    "  \\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Clearly when $\\epsilon=0$ the two columns/rows are not linear independent, and hence the determinant of this matrix is zero, the condition number is infinite, and the linear system does not have a solution (as the two equations would be telling us the contradictory information that $2x+y$ is equal to 3 and is also equal to 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Exercise 7.2: Ill-conditioned matrix</span>\n",
    "\n",
    "Consider a range of small values for $\\epsilon$ and calculate the matrix determinant and condition number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A=numpy.array([[ ],[ ]])\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find for $\\epsilon=0.001$ that $\\det(A)=0.002$ (i.e. quite a lot smaller than the other coefficients in the matrix) and $\\textrm{cond}(A)\\approx 5000$.\n",
    "\n",
    "Using `numpy.dot(linalg.inv(A),b)` you should also be able to compute the solution $\\pmb{x}=(1501.5,-3000.)^T$.\n",
    "\n",
    "What happens when you make a very small change to the coefficients of the matrix (e.g. set $\\epsilon=0.002$)?\n",
    "\n",
    "You should find that this change of just $0.1\\%$ in one of the coefficients of the matrix results in a $100%$ change in both components of the solution!\n",
    "\n",
    "This is the consequence of the matrix being ill-conditioned - we should not trust the numerical solution to ill-conditioned problems.\n",
    "\n",
    "A way to see this is to recognise that computers do not perform arithmetic exactly - they necessarily have to [truncate numbers](http://www.mathwords.com/t/truncating_a_number.htm) at a certain number of significant figures, performing multiple operations with these truncated numbers can lead to an erosion of accuracy. Often this is not a problem, but these so-called [roundoff](http://mathworld.wolfram.com/RoundoffError.html) errors in algorithms generating $A$, or operating on $A$ as in Gaussian elimination, will lead to small inaccuracies in the coefficients of the matrix. Hence, in the case of ill-conditioned problems, will fall foul of the issue seen above where a very small error in an input to the algorithm led to a far larger error in an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roundoff errors\n",
    "\n",
    "Note that in this course we have largely ignored the limitations of the floating point arithmetic performed by computers, including round-off errors.  \n",
    "\n",
    "This is often the topic of the first lecture of courses, or first chapter of books, on Numerical Methods or Numerical Analysis - do take a look at some examples if you are interested.  \n",
    "\n",
    "Also take a look at *D. Goldberg 1991: What every computer scientist should know about floating-point arithmetic, ACM Computing Surveys 23, Pages 5-48*.\n",
    "\n",
    "For some examples of catastrophic failures due to round off errors see <https://www.ma.utexas.edu/users/arbogast/misc/disasters.html> and <http://ta.twi.tudelft.nl/users/vuik/wi211/disasters.html> and [the sinking of the Sleipner A offshore platform](http://www.ima.umn.edu/~arnold/disasters/sleipner.html).\n",
    "\n",
    "As an example, consider the mathematical formula\n",
    "\n",
    "$$f(x)=(1-x)^{10}.$$\n",
    "\n",
    "We can of course relatively easily expand this out by hand\n",
    "\n",
    "$$f(x)=1- 10x + 45x^2 - 120x^3 + 210x^4 - 252x^5 + 210x^6 - 120x^7 + 45x^8 - 10x^9 + x^{10}.$$\n",
    "\n",
    "Mathematically these two expressions for $f(x)$ are identical; when evaluated by a computer different operations will be performed, which (we hope) should give the same answer. For numbers $x$ away from $1$ these two expressions do return (pretty much) the same answer.  \n",
    "\n",
    "However, for $x$ close to 1 the second expression adds and subtracts individual terms of increasing size which should largely cancel out, but they don't to sufficient accuracy due to round off errors; these errors accumulate with more and more operations, leading a loss of significance <https://en.wikipedia.org/wiki/Loss_of_significance>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00010485760000000006 0.00010485760000436464 4.1623815505431594e-11\n",
      "1.0239999999999978e-07 1.0240001356576212e-07 1.3247813024364063e-07\n",
      "9.765625000000086e-14 1.2378986724570495e-13 0.21111273343425307\n"
     ]
    }
   ],
   "source": [
    "def f1(x):\n",
    "    return (1. - x)**10\n",
    "\n",
    "def f2(x):\n",
    "    return (1. - 10.*x + 45.*x**2 - 120.*x**3 +\n",
    "           210.*x**4 - 252.*x**5 + 210.*x**6 -\n",
    "           120.*x**7 + 45.*x**8 - 10.*x**9 + x**10)\n",
    "\n",
    "x=0.6\n",
    "print(f1(x),f2(x),1.-f1(x)/f2(x)) # values computed in different ways and their relative difference\n",
    "x=0.8\n",
    "print(f1(x),f2(x),1.-f1(x)/f2(x)) \n",
    "x=0.95\n",
    "print(f1(x),f2(x),1.-f1(x)/f2(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm stability\n",
    "\n",
    "The susceptibility for a numerical algorithm to dampen (inevitable) errors, rather than to magnify them as we have seen in examples above, is termed *stability*.  This is a concern for numerical linear algebra as considered here, as well as for the numerical solution of differential equations as you will see in NM2.  In that case you don't want small errors to grow and accumulate as you propagate the solution to an ODE or PDE forward in time say.\n",
    "\n",
    "If your algorithm is not inherently stable, or has other limitation, you need to understand and appreciate this, as it can cause catastrophic failures! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct vs iterative methods\n",
    "\n",
    "Two types/families of methods exist to solve matrix systems.  These are termed *direct* methods and *iterative* (or *indirect*) methods.\n",
    "\n",
    "Direct methods perform operations on the linear equations (the matrix system), e.g. the substitution of one equation into another which we performed two weeks ago for your example $2\\times 2$ system considered in MM1. This (and the subsequent Gaussian elimination algorithm) transformed the equations making up the linear system into equivalent ones with the aim of eliminating unknowns from some of the equations and hence allowing for easy solution through back (or forward) substitution.\n",
    "\n",
    "Also, in MM1 you (may have) learnt Cramer's rule which gives an explicit formula for the inverse of a matrix, or for the solution of a linear matrix system.  It was pointed out that the computational cost (in terms of arithmetic operations required; also termed complexity) scaled like $(n+1)!$, whereas the Gaussian elimination method (which is basically the substitution method descrined above, and implemented over the previous two lectures) scaled like $n^3$.  For large $n$ Gaussian elimination will clearly be more efficient - you considered the case where $n=100$ in MM1 for example. $n$ here refers to the number of unknowns or equations, or sometimes termed the *degrees of freedom* of the problem.\n",
    "\n",
    "An advantage of direct methods is that they provide the exact solution (assuming exact arithmetic, i.e. ignoring the round off related issues mentioned above) in a finite number of operations.\n",
    "\n",
    "However, as pointed out previously, $n$ could be billions for hard-core applications such as in numerical weather forecasting. In this case the $n^3$ operations required of a direct algorithm such as Gaussian elimination is completely prohibitive. In an attempt to further reduce this cost *iterative* algorithms were devised.\n",
    "\n",
    "These algorithms start with an initial guess at the solution ($\\pmb{x}_0$), and *iteratively* improve this producing a series of approximate answers $\\pmb{x}_k$. For the *exact* answer to the matrix system $A\\pmb{x} = \\pmb{b}$, we know that the residual vector $\\pmb{r} = A\\pmb{x}-\\pmb{b}$ is zero. For our iterative procedure, we can use the norm of the residual vector $\\pmb{r}_k = A\\pmb{x}_k-\\pmb{b}$ based on the approximate solution $\\pmb{x}_k$, as a measure of how close we are to solving the equation (the norm $\\|\\pmb{r}_k\\|$ expresses this as a single number). As we iterate further, we hope to drive down this number and we may stop the iterations at some small (non-zero) residual norm tolerance level. The final iteration gives us an answer $\\pmb{x}_k$ which is still an approximation to the solution and not the exact solution we would obtain with direct methods.  The residual norm tolerance stopping criteria therefore needs to be thought about carefully, e.g. depending on how accurate a solution $\\pmb{x}$ we require.\n",
    "\n",
    "We have already considered Gaussian elimination (and back substitution) as examples of direct solution methods. We'll consider an example of an iterative method now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative methods - Jacobi's method\n",
    "\n",
    "Consider our matrix system\n",
    "\n",
    "$$A\\pmb{x}=\\pmb{b} \\quad \\iff \\quad \\sum_{j=1}^nA_{ij}x_j=b_i,\\quad \\textrm{for}\\quad i=1,2,\\ldots, n.$$\n",
    "\n",
    "Let's rewrite this by pulling out the term involving $x_i$ (i.e. for each row $i$ pull out the diagonal from the summation):\n",
    "\n",
    "$$A_{ii}x_i + \\sum_{\\substack{j=1\\\\ j\\ne i}}^nA_{ij}x_j=b_i,\\quad  i=1,2,\\ldots, n.$$\n",
    "\n",
    "We can then come up with a formula for our unknown $x_i$:\n",
    "\n",
    "$$x_i = \\frac{1}{A_{ii}}\\left(b_i- \\sum_{\\substack{j=1\\\\ j\\ne i}}^nA_{ij}x_j\\right),\\quad  i=1,2,\\ldots, n.$$\n",
    "\n",
    "Now of course for each individual $x_i$, all the other components of $\\pmb{x}$ appearing on the RHS are also unknown and so this is an example of an implicit formula which doesn't help us directly, but does suggest the following iterative scheme:\n",
    "\n",
    "* Starting from a guess at the solution $\\pmb{x}^{(0)}$\n",
    "\n",
    "* iterate for $k>0$\n",
    "$$x_i^{(k)} = \\frac{1}{A_{ii}}\\left(b_i- \\sum_{\\substack{j=1\\\\ j\\ne i}}^nA_{ij}x_j^{(k-1)}\\right),\\quad  i=1,2,\\ldots, n.$$\n",
    "\n",
    "Note that for this iteration, for a fixed $k$, it does not matter in which order we perform the operations over $i$ as the right hand side only contains the entries of $\\pmb{x}$ at the previous iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "4\n",
      "[-0.16340811 -0.01532703  0.27335262  0.36893551]\n",
      "[-0.16340816 -0.01532706  0.27335264  0.36893555]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['linalg']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10703d240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pylab\n",
    "import numpy\n",
    "\n",
    "A=numpy.array([[10., 2., 3., 5.],[1., 14., 6., 2.],[-1., 4., 16., -4],[5. ,4. ,3. ,11. ]])\n",
    "b = numpy.array([1., 2., 3., 4.])\n",
    "print(A.shape[0])\n",
    "\n",
    "# an initial guess at the solution - here just a vector of zeros of length the number of rows in A\n",
    "x = numpy.zeros(A.shape[0]) \n",
    "\n",
    "tol = 1.e-6 # iteration tolerance\n",
    "it_max = 1000 # upper limit on iterations if we don't hit tolerance\n",
    "residuals=[] # store residuals\n",
    "\n",
    "for it in range(it_max):\n",
    "    x_new = numpy.zeros(A.shape[0])  # initialise the new solution vector\n",
    "    for i in range(A.shape[0]):\n",
    "        x_new[i] = (1./A[i, i]) * (b[i] - numpy.dot(A[i, :i], x[:i]) - numpy.dot(A[i, i + 1:], x[i + 1:]))\n",
    "\n",
    "    residual = numpy.linalg.norm(numpy.dot(A, x) - b)  # calculate the norm of the residual r=Ax-b for this latest guess\n",
    "    residuals.append(residual) # store it for later plotting\n",
    "    if (residual < tol): # if less than our required tolerance jump out of the iteration and end.\n",
    "        break\n",
    "\n",
    "    x = x_new # update old solution\n",
    "\n",
    "pylab.semilogy(residuals) # plot the log of the residual against iteration number \n",
    "print(x_new) # our solution vector\n",
    "print(numpy.dot(linalg.inv(A),b))  # check against scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative methods - Gauss-Seidel's method\n",
    "\n",
    "We can make a small improvement to Jacobi's method using the updated components of the solution vector as soon as they become available:\n",
    "\n",
    "* Starting from a guess at the solution $\\pmb{x}^{(0)}$\n",
    "\n",
    "* iterate for $k>0$\n",
    "$$x_i^{(k)} = \\frac{1}{A_{ii}}\\left(b_i- \\sum_{\\substack{j=1\\\\ j< i}}^nA_{ij}x_j^{(k)} - \\sum_{\\substack{j=1\\\\ j> i}}^nA_{ij}x_j^{(k-1)}\\right),\\quad  i=1,2,\\ldots, n.$$\n",
    "\n",
    "Note that as opposed to Jacobi, we can overwrite the entries of $\\pmb{x}$ as they are updated, with Jacobi we need to store both the new as well as the old iteration (i.e. not overwrite the old entries until we have finished with them - which was not until the end of every iteration).\n",
    "\n",
    "As we are using updated knowledge immediately, the Gauss-Seidel algorithm should converge faster than Jacobi, but note that this convergence can only be *guaranteed* for matrices which are diagonally dominant (for every row, the magnitude of value on the main diagonal is greater than the sum of the magnitudes of all the other entries in that row), or if the matrix is *symmetric positive definite* (a property we won't define in this course).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Exercise 7.3: Implement Gauss-Seidel's method.</span>\n",
    "\n",
    "Generalise the Jacobi code to solve the matrix problem using Gauss-Seidel's method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse matrices\n",
    "\n",
    "Note that the matrices which result from the numerical solution of differential equations are generally  *sparse* (<https://en.wikipedia.org/wiki/Sparse_matrix>) which means that most entries are zero (the alternative is termed *dense*).  Knowing which entries are zero means that we can devise more efficient matrix storage methods, as well as more efficient implementations of the above algorithms (e.g. by not bothering to do operations that we know involve multiplications by zero - we know the answer will be zero).\n",
    "\n",
    "As an example, for the two iterative methods shown above (Jacobi and Gauss Seidel), the cost of *each* iteration is quadratically dependent on the number of unknowns $n$, since we need to loop through all the entries of the $n\\times n$ matrix $A$. For a fixed number of iterations the computational cost of these methods therefore scales as $n^2$. If we know that each row only contains a fixed, small number of non-zero entries however (as for example the matrix in the example below), we can simply skip the zero entries and the cost *per iteration* becomes linear in $n$. These scalings of $n^2$ for *dense* and $n$ for *sparse* matrices for the cost per iteration are typical for iterative methods. Unfortunately this does not mean that the overall cost of an iterative method is also $n^2$ or $n$, as the number of iterations that is needed to achieve a certain accuracy quite often also increases for larger problem sizes. The number of required iterations typically only increases very slowly however, so that the cost of the method is still considerably cheaper than direct methods, in particular for very large problems.\n",
    "\n",
    "A huge range of iterative solution methods exist and the literature on this topic is massive. Below is an example of using scipy to access the Conjugate Gradient algorithm which is a popular example of a method suitable for matrices which result from the numerical solution of differential equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.          0.89655653  0.         ...,  0.          0.          0.        ]\n",
      " [ 0.89655653 -2.          0.87177524 ...,  0.          0.          0.        ]\n",
      " [ 0.          0.87177524 -2.         ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ..., -2.          0.99194932  0.        ]\n",
      " [ 0.          0.          0.         ...,  0.99194932 -2.          0.43697999]\n",
      " [ 0.          0.          0.         ...,  0.          0.43697999 -2.        ]]\n",
      "  (0, 0)\t-2.0\n",
      "  (0, 1)\t0.896556533301\n",
      "  (1, 0)\t0.896556533301\n",
      "  (1, 1)\t-2.0\n",
      "  (1, 2)\t0.871775239883\n",
      "  (2, 1)\t0.871775239883\n",
      "  (2, 2)\t-2.0\n",
      "  (2, 3)\t0.819582488877\n",
      "  (3, 2)\t0.819582488877\n",
      "  (3, 3)\t-2.0\n",
      "  (3, 4)\t0.1564802252\n",
      "  (4, 3)\t0.1564802252\n",
      "  (4, 4)\t-2.0\n",
      "  (4, 5)\t0.275015259956\n",
      "  (5, 4)\t0.275015259956\n",
      "  (5, 5)\t-2.0\n",
      "  (5, 6)\t0.106480082203\n",
      "  (6, 5)\t0.106480082203\n",
      "  (6, 6)\t-2.0\n",
      "  (6, 7)\t0.822730120579\n",
      "  (7, 6)\t0.822730120579\n",
      "  (7, 7)\t-2.0\n",
      "  (7, 8)\t0.984812620065\n",
      "  (8, 7)\t0.984812620065\n",
      "  (8, 8)\t-2.0\n",
      "  :\t:\n",
      "  (41, 41)\t-2.0\n",
      "  (41, 42)\t0.232846001045\n",
      "  (42, 41)\t0.232846001045\n",
      "  (42, 42)\t-2.0\n",
      "  (42, 43)\t0.194107745952\n",
      "  (43, 42)\t0.194107745952\n",
      "  (43, 43)\t-2.0\n",
      "  (43, 44)\t0.830229172381\n",
      "  (44, 43)\t0.830229172381\n",
      "  (44, 44)\t-2.0\n",
      "  (44, 45)\t0.0118443060111\n",
      "  (45, 44)\t0.0118443060111\n",
      "  (45, 45)\t-2.0\n",
      "  (45, 46)\t0.832301658591\n",
      "  (46, 45)\t0.832301658591\n",
      "  (46, 46)\t-2.0\n",
      "  (46, 47)\t0.852272764412\n",
      "  (47, 46)\t0.852272764412\n",
      "  (47, 47)\t-2.0\n",
      "  (47, 48)\t0.991949320832\n",
      "  (48, 47)\t0.991949320832\n",
      "  (48, 48)\t-2.0\n",
      "  (48, 49)\t0.436979991321\n",
      "  (49, 48)\t0.436979991321\n",
      "  (49, 49)\t-2.0\n",
      "1 2.61195585721\n",
      "2 1.33143037539\n",
      "3 0.733978842192\n",
      "4 0.411287653437\n",
      "5 0.20375820891\n",
      "6 0.104416839914\n",
      "7 0.0516217557408\n",
      "8 0.0269290257855\n",
      "9 0.0106110559925\n",
      "10 0.00500398493804\n",
      "11 0.00219461425849\n",
      "12 0.000931047731175\n",
      "13 0.000453462440388\n",
      "14 0.000189723670259\n",
      "15 9.18022390046e-05\n",
      "16 3.59963521627e-05\n",
      "17 1.4020295499e-05\n",
      "18 6.15101549938e-06\n",
      "19 2.19477966813e-06\n",
      "20 8.85978530449e-07\n",
      "21 3.0108447406e-07\n",
      "22 1.02906922556e-07\n",
      "23 4.08910290447e-08\n",
      "24 1.08351470579e-08\n",
      "25 3.6091377661e-09\n",
      "26 1.37771048413e-09\n",
      "27 4.2624315766e-10\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "n=50\n",
    "main_diag = numpy.ones(n)   #just a vector of ones\n",
    "off_diag = numpy.random.random(n-1)  # to make it a bit more interesting make the off-diagonals random\n",
    "A = numpy.diag(-2*main_diag,0) + numpy.diag(1.*off_diag,1) + numpy.diag(1.*off_diag,-1)\n",
    "# A random RHS vector\n",
    "b = numpy.random.random(A.shape[0])\n",
    "\n",
    "print(A) # print our A in \"dense\" matrix format\n",
    "\n",
    "sA = scipy.sparse.csr_matrix(A) # The same matrix in a \"sparse\" matrix data structure where only non-zeros stored\n",
    "print(sA)\n",
    "\n",
    "# now use a scipy iterative algorithm (Conjugate Gradient) to solve\n",
    "\n",
    "# First define a (callback) function which we are allowed to pass to the solver; here this is coded such that it will store and print the iteration numbers and residuals - basically a method to output some diagnostic information on the solver as it executes\n",
    "def gen_callback_cg():\n",
    "    diagnostics = dict(it=0, residuals=[]) \n",
    "    def callback(xk):   # xk is the solution computed by CG at each iteration\n",
    "        diagnostics[\"it\"] += 1\n",
    "        diagnostics[\"residuals\"].append(numpy.linalg.norm(numpy.dot(A, xk) - b))\n",
    "        print(diagnostics[\"it\"], numpy.linalg.norm(numpy.dot(A, xk) - b))\n",
    "    return callback    \n",
    "\n",
    "x_sol = scipy.sparse.linalg.cg(A,b,x0=None, tol=1e-10, maxiter=1000, callback=gen_callback_cg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## An example of a sparse matrix\n",
    "\n",
    "Let us consider an electric circuit arranged in a regular grid of $n$ rows and $m$ columns. The nodes in the grid are numbered from 0 to $nm-1$ as indicated in the diagram below.\n",
    "\n",
    "![bla](images/circuit.png)\n",
    "\n",
    "We want to calculate the electric potential $V_i$ in all of the nodes $i$. A node $i$ somewhere in the middle of the circuit is connected via resistor to nodes $i-1$ and $i+1$ to the left and right respectively, and to the nodes $i-m$ and $i+m$ in the rows above and below. For simplicity we assume that all resistors have the same resistance value $R$. The first and last node of the circuit (0 and $nm-1$) to a battery via two additional resistors, with the same resistance value $R$.\n",
    "\n",
    "The sum of the currents coming into a node is zero (if we use a sign convention where a current coming into a node is positive and a current going out is negative. The currents between two nodes can be calculated using Ohm's law: $I=V/R$ where $R$ is the resistance of the resistor, and $V$ is the potential difference between two nodes, say $V=V_i-V_{i-1}$. Therefore we can write:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  0 &=& I_{i-1\\to i} + I_{i+1\\to i} + I_{i-m\\to i} + I_{i+m\\to i} \\\\\n",
    "    &=& V_{i-1\\to i}/R + V_{i+1\\to i}/R + V_{i-m\\to i}/R + V_{i+m\\to i}/R \\\\\n",
    "    &=& (V_{i}-V_{i-1})/R + (V_{i}-V_{i+1})/R + (V_{i}-V_{i-m})/R + (V_{i}-V_{i+m})/R \\\\\n",
    "    &=& (4V_{i}-V_{i-1}-V_{i+1}-V_{i-m}-V_{i+m})/R\n",
    "\\end{eqnarray}\n",
    "\n",
    "This gives us one linear equation for each node in the circuit (with slight modifications for nodes that are not in the interior). These can be combined into a linear system $Ax=b$ which is assembled in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4 # number of rows\n",
    "m = 3 # number of columns\n",
    "V_battery = 5.0 # voltage on the right of the battery\n",
    "\n",
    "A = numpy.zeros((n*m, n*m))\n",
    "for row in range(n):\n",
    "    for column in range(m):\n",
    "        i = row*m + column # node number\n",
    "        if column>0: # left neighbour\n",
    "            A[i,i-1] += -1.0\n",
    "            A[i,i] += 1.0\n",
    "        if column<m-1: # right neighbour\n",
    "            A[i,i+1] += -1.0\n",
    "            A[i,i] += 1.0\n",
    "        if row>0: # neighbour above\n",
    "            A[i,i-m] += -1.0\n",
    "            A[i,i] += 1.0\n",
    "        if row<n-1: # neighbour below\n",
    "            A[i,i+m] += -1.0\n",
    "            A[i,i] += 1.0\n",
    "\n",
    "# connecting node 0 to the battery: I = (V_0 - 0)/R\n",
    "A[0,0] += 1.0 \n",
    "# connecting last node nm-1 to the battery: I = (V_0 - V_battery)/R = V_0/R - V_battery/R\n",
    "A[n*m-1,n*m-1] += 1.0\n",
    "# the V_battery/R term is a constant that does not depend on the unknowns, so ends up in the rhs vector b\n",
    "b = numpy.zeros(n*m)\n",
    "b[n*m-1] = V_battery",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
