{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Methods 1\n",
    "### [Gerard Gorman](http://www.imperial.ac.uk/people/g.gorman), [Matthew Piggott](http://www.imperial.ac.uk/people/m.d.piggott)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5: Numerical Differentiation\n",
    "\n",
    "## Learning objectives:\n",
    "\n",
    "* Learn about finite difference approximations to derivatives.\n",
    "* Be able to implement forward and central difference methods.\n",
    "* Calculate higher-order derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward differencing\n",
    "\n",
    "Approximations to the derivatives of a function can be computed by using weighted sums of function evaluations at a number of points. The elementary definition of the derivative of a function $f$ at a point $x_0$ is given by:\n",
    "\n",
    " $$ f'(x_0)=\\lim_{h\\rightarrow 0} \\frac{f(x_0+h)-f(x_0)}{h} $$\n",
    "\n",
    "We can turn this into an approximation rule for $f'(x)$ by replacing the limit as $h$ approaches $0$ with a small but finite $h$:\n",
    "\n",
    " $$ f'(x_0)\\approx \\frac{f(x_0+h)-f(x)}{h},\\qquad h>0 $$\n",
    "\n",
    "The figure below illustrates this approximation. Because the approximate gradient is calculated using values of $x$ greater than $x_0$, this algorithm is known as the **forward difference method**. In the figure the derivative is approximated by the slope of the red line, while the true derivative is the slope of the blue line -- if the second (and/or higher) derivative of the function is large then this approximation might not be very good unless you make $h$ very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Forward difference method for approximating $f'(x_0)$. The derivative is approximated by the slope of the red line, while the true derivative is the slope of the blue line.](https://raw.githubusercontent.com/ggorman/Numerical-methods-1/master/notebook/images/forward_diff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taylor series to estimate accuracy\n",
    "We can use a [Taylor series expansion](http://mathworld.wolfram.com/TaylorSeries.html) to estimate the accuracy of the method. We can write:\n",
    "\n",
    "\\begin{align*}\n",
    "f(x_0+h)&=f(x_0)+hf'(x_0)+ \\frac{h^2}{2!}f''(x_0) + \\frac{h^3}{3!}f'''(x_0) + \\ldots\\\\ & =f(x_0)+hf'(x_0)+O(h^2)\n",
    "\\end{align*}\n",
    " \n",
    "where $O(h^2)$ represents the collection of terms that are second-order in $h$ or higher.\n",
    "\n",
    "If we rearrange this expression to isolate the gradient term $f'(x_0)$ on the left hand side, we find:\n",
    "\n",
    " $$ hf'(x_0)=f(x_0+h)-f(x_0) +O(h^2) $$\n",
    " \n",
    "and therefore, by dividing through by $h$,\n",
    " \n",
    " $$ f'(x_0)=\\frac{f(x_0+h)-f(x_0)}{h}+O(h) $$\n",
    "\n",
    "As we are left with $O(h)$ at the end, we know that the forward difference method is first-order (i.e. $h^1$) -- as we make the spacing $h$ smaller we expect the error in our derivative to fall linearly.\n",
    "\n",
    "For general numerical methods we generally strive for something better than this -- if we halve our $h$ (and so are doing twice as much (or more) work potentially) we would like our error to drop super-linearly: i.e. by a factor of 4 (second-order method) or 8 (third-order method) or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Exercise 5.1: Compute first derivative using forward differencing</span>\n",
    "\n",
    "Use the forward difference scheme to compute an approximation to $f'(2.36)$ from the following data:\n",
    "\n",
    "$f(2.36) = 0.85866$\n",
    "\n",
    "$f(2.37) = 0.86289$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.423\n"
     ]
    }
   ],
   "source": [
    "h = 2.37-2.36\n",
    "df = (0.86289-0.85866)/h\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central differencing\n",
    "\n",
    "In an attempt to derive a more accurate method, we use two Taylor series expansions; one in the positive $x$ direction from $x_0$, and one in the negative direction. Because we hope to achieve better than first order, we include an extra term in the series:\n",
    "\n",
    "$$ f(x_0+h)=f(x_0)+hf'(x_0)+\\frac{h^2}{2}f''(x_0) + O(h^3) $$\n",
    "\n",
    "$$ f(x_0-h)=f(x_0)-hf'(x_0)+\\frac{(-h)^2}{2}f''(x_0) + O((-h)^3) $$\n",
    "\n",
    "Using the fact that $(-h)^2=h^2$ and the absolute value signs from the definition of $O$, this is equivalent to:\n",
    "\n",
    "$$ f(x_0+h)=f(x_0)+hf'(x_0)+\\frac{h^2}{2}f''(x_0) + O(h^3) $$\n",
    "  \n",
    "$$ f(x_0-h)=f(x_0)-hf'(x_0)+\\frac{h^2}{2}f''(x_0) + O(h^3) $$\n",
    "  \n",
    "Remember that we are looking for an expression for $f'(x_0)$. Noticing the sign change between the derivative terms in the two equations, we subtract the bottom equation from the top equation to give:\n",
    "\n",
    "$$ f(x_0+h)-f(x_0-h)=2hf'(x_0) + O(h^3) $$\n",
    "\n",
    "Finally, rearrange to get an expression for $f'(x_0)$:\n",
    "\n",
    "$$ f'(x_0)=\\frac{f(x_0+h)-f(x_0-h)}{2h} + O(h^2) $$\n",
    "\n",
    "We can see that by taking an interval symmetric about $x_0$, we have created a second-order approximation for the derivative of $f$. This symmetry gives the scheme its name: the central difference method. The figure below illustrates this scheme. The derivative is approximated by the slope of the red line, while the true derivative is the slope of the blue line.  \n",
    "\n",
    "Even without the analysis above it's hopefully clear visually why this should in general give a lower error than the forward difference approach. However the analysis of the two methods does tell us that as we halve $h$ the error should drop by a factor 4 rather than the 2 we get for the first-order forward differencing.\n",
    "\n",
    "![\"Central difference method for approximating $f'(x_0)$. The derivative is approximated by the slope of the red line, while the true derivative is the slope of the blue line.\"](https://raw.githubusercontent.com/ggorman/Numerical-methods-1/master/notebook/images/central_diff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Exercise 5.2: Compute first derivative using central differencing</span>\n",
    "\n",
    "Use the data below to compute $f'(0.2)$ using central differencing:\n",
    "\n",
    "$$f(0.1) = 0.078348$$\n",
    "$$f(0.2) = 0.138910$$\n",
    "$$f(0.3) = 0.192916$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57284\n"
     ]
    }
   ],
   "source": [
    "h=0.1\n",
    "df = (0.192916-0.078348)/(2*h)\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Exercise 5.3: Compute the derivative of $\\sin(x)$</span>\n",
    "\n",
    "Compute $\\frac{d(\\sin x)}{dx}$ at $x = 0.8$ using (a) forward differencing and (b) central differencing. Experiment with the value of $h$ to obtain the most accurate result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact deriviative at sin(0.8) =  0.696706709347\n",
      "Forward differencing                    Central differencing\n",
      "  0.256492 (error=      0.44)           0.586258 (error=      0.11)\n",
      "  0.492404 (error=       0.2)           0.668038 (error=     0.029)\n",
      "  0.600269 (error=     0.096)           0.689472 (error=    0.0072)\n",
      "  0.650117 (error=     0.047)           0.694894 (error=    0.0018)\n",
      "  0.673843 (error=     0.023)           0.696253 (error=   0.00045)\n",
      "  0.685386 (error=     0.011)           0.696593 (error=   0.00011)\n",
      "  0.691074 (error=    0.0056)           0.696678 (error=   2.8e-05)\n",
      "  0.693897 (error=    0.0028)             0.6967 (error=   7.1e-06)\n",
      "  0.695304 (error=    0.0014)           0.696705 (error=   1.8e-06)\n",
      "  0.696006 (error=    0.0007)           0.696706 (error=   4.4e-07)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def forward_diff(f, x, h):\n",
    "    fx = f(x)\n",
    "    fxph = f(x+h)\n",
    "    return (fxph-fx)/h\n",
    "\n",
    "def central_diff(f, x, h):\n",
    "    fxph = f(x+h)\n",
    "    fxnh = f(x-h)\n",
    "    return (fxph-fxnh)/(2*h)\n",
    "\n",
    "exact = math.cos(0.8)\n",
    "\n",
    "print \"Exact deriviative at sin(0.8) = \", math.cos(0.8)\n",
    "print \"%20s%40s\"%(\"Forward differencing\", \"Central differencing\")\n",
    "h=1.0\n",
    "for i in range(10):\n",
    "    fd = forward_diff(math.sin, 0.8, h)\n",
    "    cd = central_diff(math.sin, 0.8, h)\n",
    "    print \"%10g (error=%10.2g)         %10g (error=%10.2g)\"%(fd, abs(fd-exact), cd, abs(cd-exact))\n",
    "    h=h/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Exercise 5.?: Write a function to perform numerical differentiation</span>\n",
    "\n",
    "As covered above, the formula\n",
    "$$f^\\prime(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}$$\n",
    "can be used to find an approximate derivative of a mathematical function f(x) if h is small. \n",
    "\n",
    "Write a function *diff*( *f*, *x*, *h*=1E-6) that returns the approximation of the derivative of a mathematical function represented by a Python function f(x).\n",
    "\n",
    "Apply the above formula to differentiate $f(x) = e^x$ at x = 0, $f(x) = e^{âˆ’2x}$ at\n",
    "x = 0, $f(x) = \\cos(x)$ at x = 2$\\pi$ , and $f(x) = \\ln(x)$ at x = 1. \n",
    "\n",
    "Use h = 0.01. In each case, write out the error, i.e., the difference between the exact derivative and the result of the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-763126f9c60b>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-763126f9c60b>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    print \"The approximate derivative of exp(x) at x = 0 is: %f. The error is %f.\" % (derivative, abs(derivative - 1)) # The 'abs' function returns the absolute value.\u001b[0m\n\u001b[1;37m                                                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Write a function for numerical differentiation\n",
    "\n",
    "from math import exp, cos, log, pi\n",
    "\n",
    "def diff(f, x, h = 1E-6):\n",
    "   numerator = f(x + h) - f(x - h)\n",
    "   derivative = numerator/(2.0*h)\n",
    "   return derivative\n",
    "   \n",
    "h = 0.01 # The step size\n",
    "\n",
    "x = 0\n",
    "f = exp\n",
    "derivative = diff(f, x, h)\n",
    "print \"The approximate derivative of exp(x) at x = 0 is: %f. The error is %f.\" % (derivative, abs(derivative - 1)) # The 'abs' function returns the absolute value.\n",
    "\n",
    "x = 0\n",
    "# Here it is not possible to simply pass in the math module's exp function,\n",
    "# so we need to define our own function instead.\n",
    "def g(x):\n",
    "   return exp(-2*x)\n",
    "f = g\n",
    "derivative = diff(f, x, h)\n",
    "print \"The approximate derivative of exp(-2*x) at x = 0 is: %f. The error is %f.\" % (derivative, abs(derivative - (-2.0)))\n",
    "\n",
    "x = 2*pi\n",
    "f = cos\n",
    "derivative = diff(f, x, h)\n",
    "print \"The approximate derivative of cos(x) at x = 2*pi is: %f. The error is %f.\" % (derivative, abs(derivative - 0))\n",
    "\n",
    "x = 1\n",
    "f = log # By default, log(x) is the natural log (i.e. the log to the base 'e')\n",
    "derivative = diff(f, x, h)\n",
    "print \"The approximate derivative of ln(x) at x = 0 is: %f. The error is %f.\" % (derivative, abs(derivative - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating second derivatives\n",
    "\n",
    "Numerical differentiation may be extended to the second derivative by noting that the second derivative is the derivative of the first derivative. That is, if we define:\n",
    "\n",
    "$$ g(x)=f'(x) $$\n",
    "\n",
    "then\n",
    "\n",
    "$$ f''(x)=g'(x) $$\n",
    "\n",
    "We have noted above that the central difference method, being second order, is superior to the forward difference method so we will choose to extend that.\n",
    "\n",
    "In order to calculate $f''(x_0)$ using a central difference method, we first calculate $f'(x)$ for each of two half intervals, one to the left of $x_0$ and one to the right:\n",
    "\n",
    "$$ f'\\left(x_0+\\frac{h}{2}\\right)\\approx\\frac{f(x_0+h)-f(x_0)}{h} $$\n",
    "$$  f'\\left(x_0-\\frac{h}{2}\\right)\\approx\\frac{f(x_0)-f(x_0-h)}{h} $$\n",
    "\n",
    "We can now calculate the second derivative using these two values. Note that we know $f'(x)$ at the points $x_0\\pm{h}/{2}$, which are only $h$ rather than $2h$ apart. Hence:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f''(x_0)&\\approx\\frac{f'(x_0+\\frac{h}{2})-f'(x_0-\\frac{h}{2})}{h}\\\\\n",
    "    &\\approx\\frac{\\frac{f(x_0+h)-f(x_0)}{h}-\\frac{f(x_0)-f(x_0-h)}{h}}{h}\\\\\n",
    "    &\\approx\\frac{f(x_0+h)-2f(x_0)+f(x_0-h)}{h^2}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Exercise 5.4: Compute second derivative</span>\n",
    "\n",
    "Calculate the second derivative $f''$ at $x = 1$ using the data below:\n",
    "\n",
    "$f(0.84) = 0.431711$\n",
    "\n",
    "$f(0.92) = 0.398519$\n",
    "\n",
    "$f(1.00) = 0.367879$\n",
    "\n",
    "$f(1.08) = 0.339596$\n",
    "\n",
    "$f(1.16) = 0.313486$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003682812499999996\n"
     ]
    }
   ],
   "source": [
    "h = 0.8\n",
    "ddf = (0.339596 - 2*0.367879 + 0.398519)/(h*h)\n",
    "print ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-central differencing\n",
    "\n",
    "In this particular case we were given more data than we actually used. An alternative approach would be to use non-centred differencing, e.g. the following is also a valid approximation to the second derivative\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f''(x_0)\\approx\\frac{f(x_0+2h)-2f(x_0+h)+f(x_0)}{h^2}\n",
    "\\end{align}$$\n",
    "\n",
    "This can come in handy if we need to approximate the value of derivatives at or near to a boundary where we don't have data beyond that boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical methods for ODEs\n",
    "\n",
    "One of the most important applications of numerical mathematics in the sciences is the numerical solution of ordinary differential equations (ODEs). This is a vast topic which rapidly becomes somewhat advanced, so we will restrict ourselves here to a very brief introduction to the solution of first order ODEs. A much more comprehensive treatment of this subject is to be found in the Numerical Methods 2 module.\n",
    "\n",
    "Suppose we have the general first order ODE:\n",
    "\n",
    "\\begin{align}\n",
    "x'(t)&=f(x,t) \\\\\n",
    "x(t_0)&=x_0\n",
    "\\end{align}\n",
    "\n",
    "That is, the derivative of $x$ with respect to $t$ is some known function of $x$ and $t$, and we also know the initial condition of $x$ at some initial time $t_0$.\n",
    "\n",
    "If we manage to solve this equation analytically, the solution will be a function $x(t)$ which is defined for every $t>t_0$. In common with all of the numerical methods we have encountered in this module, our objective will be to find an approximate solution to the ODE at a finite set of points. In this case, we will attempt to find approximate solutions at $t=t_0,t_0+h,t_0+2h,t_0+3h,\\ldots$.\n",
    "\n",
    "It is frequently useful to think of the independent variable, $t$, as representing time. A numerical method steps forward in time units of $h$, attempting to calculate $x(t+h)$ in using the previously calculated value $x(t)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euler's method\n",
    "\n",
    "To derive a numerical method, we can first turn once again to the Taylor\n",
    "series. In this case, we could write:\n",
    "\n",
    "$$ x(t+h)=x(t)+h x'(t) + O(h^2) $$\n",
    "\n",
    "Using the definition of our ODE above, we can substitute in for $x'(t)$:\n",
    "\n",
    "$$ x(t+h)=x(t)+h f(t,x(t))+ O(h^2)$$\n",
    "\n",
    "Notice that the value of $x$ used in the evaluation of $f$ is that at time $t$. This simple scheme is named **Euler's method** after the 18th century Swiss mathematician, Leonard Euler. \n",
    "\n",
    "The fact that the function $f$ in this relation is evaluated at the old time level $t$ means that this is what is known\n",
    "as an explicit method -- i.e. we have all the information required at time $t$ to explicitly compute the right-hand-side,\n",
    "and hence easily find the new value for $x(t+h)$. This form of the method is therefore more correctly called either Explicit Euler or Forward Euler.  We could also evaluate the RHS at some time between $t$ and $t+h$ (in the case of $t+h$ this method is called Implicit or Backward Euler) this is more complex to solve for the new $x(t+h)$ but can have advantageous accuracy and stability properties.\n",
    "\n",
    "The formula given is used to calculate the value of $x$ one time step forward from the last known value. The error is therefore the local truncation error. If we actually wish to know the value at some fixed time $T$ then we will have to calculate $(T-t_0)/h$ steps of the method. This sum over $O(1/h)$ steps results in a global truncation error for Euler's method of $O(h)$. In other words, Euler's method is first order.\n",
    "\n",
    "To illustrate Euler's method, and convey the fundamental idea of all time stepping methods, we'll use Euler's method to solve one of the simplest of all ODEs:\n",
    "\n",
    "$$ x'(t)=x $$\n",
    "$$ x(0)=1 $$\n",
    "\n",
    "We know, of course, that the solution to this equation is $x=e^t$, but let's ignore that for one moment and evaluate $x(0.1)$ using Euler's method with steps of $0.05$. The first step is:\n",
    "\n",
    "$$\\begin{align}\n",
    "  x(0.05)&\\approx x(0)+0.05x'(0)\\\\\n",
    "  &\\approx1+.05\\times1\\\\\n",
    "  &\\approx 1.05\n",
    "\\end{align}$$\n",
    "\n",
    "Now that we know $x(0.05)$, we can calculate the second step:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  x(0.1)&\\approx x(0.05)+0.05x(0.05)\\\\\n",
    "  &\\approx 1.05+.05\\times1.05\\\\\n",
    "  &\\approx 1.1025\n",
    "\\end{align}$$\n",
    "\n",
    "Now the actual value of $e^{0.1}$ is around $1.1051$ so we're a couple of percent off even over a very short time interval and only a couple of steps of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Exercise 5.?: Implementing Forward Euler's method</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heun's method\n",
    "\n",
    "Euler's method is first order because it calculates the derivative using only the information available at the beginning of the time step. As we observed previously, higher order convergence can be obtained if we also employ information from other points in the interval. Heun's method may be derived by attempting to use derivative information at both the start and the end of the interval:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  x(t+h)&\\approx x(t)+\\frac{h}{2}\\left(x'(t)+x'(t+h)\\right)\\\\\n",
    "  &\\approx x(t)+\\frac{h}{2}\\big(f(x,t)+f(t+h,x(t+h))\\big)\n",
    "\\end{align}$$\n",
    "\n",
    "The difficulty with this approach is that we now require $x(t+h)$ in order to calculate the final term in the equation, and that's what we set out to calculate so we don't know it yet! The solution to this dilemma adopted in Heun's method is to use a first guess at $x(t+h)$ calculated using Euler's method:\n",
    "\n",
    "$$ \\tilde{x}(t+h)=x(t)+hf(x,t) $$\n",
    "\n",
    "This first guess is then used to solve for $x(t+h)$ using:\n",
    "\n",
    "$$ x(t+h)\\approx x(t)+\\frac{h}{2}\\big(f(x,t)+f(t+h,\\tilde{x}(t+h))\\big)$$\n",
    "\n",
    "The generic term for schemes of this type is **predictor-corrector**. The initial calculation of $\\tilde{x}(t+h)$ is used to predict the new value of $x$ and then this is used in a more accurate calculation to produce a more correct value. \n",
    "\n",
    "Note that Heun's method is $O(h^2)$, i.e. a second-order method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Exercise 5.?: Implementing Heun's method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Methods II\n",
    "\n",
    "Note that you will do a lot more on the numerical solution of ODEs (and also extend to the solution of PDEs) in the Numerical Methods II course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
