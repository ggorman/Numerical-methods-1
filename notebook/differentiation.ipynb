{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Numerical Methods 1\n",
      "### [Gerard Gorman](http://www.imperial.ac.uk/people/g.gorman), [Matthew Piggott](http://www.imperial.ac.uk/people/m.d.piggott), [Christian Jacobs](http://www.christianjacobs.uk)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Lecture 3: Numerical Differentiation\n",
      "\n",
      "## Learning objectives:\n",
      "\n",
      "* Learn about finite difference approximations to derivatives.\n",
      "* Be able to implement forward and central difference methods.\n",
      "* Calculate higher-order derivatives."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Forward differencing\n",
      "\n",
      "Approximations to the derivatives of a function can be computed by using weighted sums of function evaluations at a number of points. The elementary definition of the derivative of a function $f$ at a point $x_0$ is given by:\n",
      "\n",
      " $$ f'(x_0)=\\lim_{h\\rightarrow 0} \\frac{f(x+h)-f(x)}{h} $$\n",
      "\n",
      "We can turn this into an approximation rule for $f'(x)$ by replacing the limit as $h$ approaches $0$ with a small but finite $h$:\n",
      "\n",
      " $$ f'(x_0)\\approx \\frac{f(x_0+h)-f(x)}{h},\\qquad h>0 $$\n",
      "\n",
      "The figure below illustrates this approximation. Because the approximate gradient is calculated using values of $x$ greater than $x_0$, this algorithm is known as the **forward difference method**.\n",
      "\n",
      "![\"Forward difference method for approximating $f'(x_0)$. The derivative is approximated by the slope of the red line, while the true derivative is the slope of the blue line.\"](https://raw.githubusercontent.com/ggorman/Numerical-methods-1/master/notebook/images/forward_diff.png)\n",
      "#####*Forward difference method for approximating $f'(x_0)$. The derivative is approximated by the slope of the red line, while the true derivative is the slope of the blue line.*\n",
      "\n",
      "So how accurate is this method? For this we can use a Taylor series expansion. We can write:\n",
      "\n",
      " $$ f(x_0+h)=f(x_0)+hf'(x_0)+O(h^2) $$\n",
      " \n",
      "where $O(h^2)$ represents the collection of terms that are second-order in $h$ or higher.\n",
      "\n",
      "If we rearrange this expression to isolate the gradient term $f'(x_0)$ on the left hand side, we find:\n",
      "\n",
      " $$ hf'(x_0)=f(x_0+h)-f(x_0) +O(h^2) $$\n",
      " \n",
      "and therefore, by dividing through by $h$,\n",
      " \n",
      " $$ f'(x_0)=\\frac{f(x_0+h)-f(x_0)}{h}+O(h) $$\n",
      "\n",
      "So the forward difference method is first order. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <span style=\"color:blue\">Exercise: Compute first derivative using forward differencing</span>\n",
      "\n",
      "Use the forward difference scheme to compute an approximation to $f'(2.36)$ from the following data:\n",
      "\n",
      "$f(2.36) = 0.85866$\n",
      "\n",
      "$f(2.37) = 0.86289$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Central differencing\n",
      "\n",
      "A first order method is not very impressive in terms of accuracy. In an attempt to derive a more accurate method, we'll try using two Taylor series expansions; one in the positive $x$ direction from $x_0$, and one in the negative direction. Because we hope to achieve better than first order, we include an extra term in the series:\n",
      "\n",
      "$$ f(x_0+h)=f(x_0)+hf'(x_0)+\\frac{h^2}{2}f''(x_0) + O(h^3) $$\n",
      "\n",
      "$$ f(x_0-h)=f(x_0)-hf'(x_0)+\\frac{(-h)^2}{2}f''(x_0) + O((-h)^3) $$\n",
      "\n",
      "Using the fact that $(-h)^2=h^2$ and the absolute value signs from the definition of $O$, this is equivalent to:\n",
      "\n",
      "$$ f(x_0+h)=f(x_0)+hf'(x_0)+\\frac{h^2}{2}f''(x_0) + O(h^3) $$\n",
      "  \n",
      "$$ f(x_0-h)=f(x_0)-hf'(x_0)+\\frac{h^2}{2}f''(x_0) + O(h^3) $$\n",
      "  \n",
      "Remember that we are looking for an expression for $f'(x_0)$. Noticing the sign change between the derivative terms in the two equations, we subtract the bottom equation from the top equation to give:\n",
      "\n",
      "$$ f(x_0+h)-f(x_0-h)=2hf'(x_0) + O(h^3) $$\n",
      "\n",
      "Rearranging a little, we have:\n",
      "\n",
      "$$ f'(x_0)=\\frac{f(x_0+h)-f(x_0-h)}{2h} + O(h^2) $$\n",
      "\n",
      "This indicates that we have been successful: by taking an interval symmetric about $x_0$, we have created a second order approximation for the derivative of $f$. This symmetry gives the scheme its name: the central difference method. The figure below illustrates this scheme.\n",
      "\n",
      "![\"Central difference method for approximating $f'(x_0)$. The derivative is approximated by the slope of the red line, while the true derivative is the slope of the blue line.\"](https://raw.githubusercontent.com/ggorman/Numerical-methods-1/master/notebook/images/central_diff.png)\n",
      "#####*Central difference method for approximating $f'(x_0)$. The derivative is approximated by the slope of the red line, while the true derivative is the slope of the blue line.*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <span style=\"color:blue\">Exercise: Compute first derivative using central differencing</span>\n",
      "\n",
      "Use the data below to compute $f'(0.2)$ using central differencing:\n",
      "\n",
      "$$f(0.1) = 0.078348$$\n",
      "$$f(0.2) = 0.138910$$\n",
      "$$f(0.3) = 0.192916$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <span style=\"color:blue\">Exercise: Compute the derivative of $\\sin(x)$</span>\n",
      "\n",
      "Compute $\\frac{d(\\sin x)}{dx}$ at $x = 0.8$ using (a) forward differencing and (b) central differencing. Experiment with the value of $h$ to obtain the most accurate result."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Calculating second derivatives\n",
      "\n",
      "Numerical differentiation may be extended to the second derivative by noting that the second derivative is the derivative of the first derivative. That is, if we define:\n",
      "\n",
      "$$ g(x)=f'(x) $$\n",
      "\n",
      "then\n",
      "\n",
      "$$ f''(x)=g'(x) $$\n",
      "\n",
      "We have noted above that the central difference method, being second order, is superior to the forward difference method so we will choose to extend that.\n",
      "\n",
      "In order to calculate $f''(x_0)$ using a central difference method, we first calculate $f'(x)$ for each of two half intervals, one to the left of $x_0$ and one to the right:\n",
      "\n",
      "$$ f'(x_0+\\frac{h}{2})\\approx\\frac{f(x_0+h)-f(x_0)}{h} $$\n",
      "$$  f'(x_0-\\frac{h}{2})\\approx\\frac{f(x_0)-f(x_0-h)}{h} $$\n",
      "\n",
      "We can now calculate the second derivative using these two values. Note that we know $f'(x)$ at the points $x_0\\pm\\frac{h}{2}$, which are only $h$ rather than $2h$ apart. Hence:\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "    f''(x_0)&\\approx\\frac{f'(x_0+\\frac{h}{2})-f'(x_0-\\frac{h}{2})}{h}\\\\\n",
      "    &\\approx\\frac{\\frac{f(x_0+h)-f(x_0)}{h}-\\frac{f(x_0)-f(x_0-h)}{h}}{h}\\\\\n",
      "    &\\approx\\frac{f(x_0+h)-2f(x_0)+f(x_0-h)}{h^2}\n",
      "\\end{align}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <span style=\"color:blue\">Exercise: Compute second derivative</span>\n",
      "\n",
      "Calculate the second derivative $f''$ at $x = 1$ using the data below:\n",
      "\n",
      "$f(0.84) = 0.431711$\n",
      "\n",
      "$f(0.92) = 0.398519$\n",
      "\n",
      "$f(1.00) = 0.367879$\n",
      "\n",
      "$f(1.08) = 0.339596$\n",
      "\n",
      "$f(1.16) = 0.313486$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Numerical methods for ODEs\n",
      "\n",
      "One of the most important applications of numerical mathematics in the sciences is the numerical solution of ordinary differential equations (ODEs). This is a vast topic which rapidly becomes somewhat advanced, so we will restrict ourselves here to a very brief introduction to the solution of first order ODEs. A much more comprehensive treatment of this subject is to be found in the Numerical Methods 2 module.\n",
      "\n",
      "Suppose we have the general first order ODE:\n",
      "\n",
      "$$ x'(t)=f(x,t) $$\n",
      "$$ x(t_0)=x_0 $$\n",
      "\n",
      "That is, the derivative of $x$ with respect to $t$ is some known function of $x$ and $t$, and we also know the initial condition of $x$ at some initial time $t_0$.\n",
      "\n",
      "If we manage to solve this equation analytically, the solution will be a function $x(t)$ which is defined for every $t>t_0$. In common with all of the numerical methods we have encountered in this module, our objective will be to find an approximate solution to the ODE at a finite set of points. In this case, we will attempt to find approximate solutions at $t=t_0,t_0+h,t_0+2h,t_0+3h,\\ldots$.\n",
      "\n",
      "It is frequently useful to think of the independent variable, $t$, as representing time. A numerical method steps forward in time units of $h$, attempting to calculate $x(t+h)$ in using the previously calculated value $x(t)$. \n",
      "\n",
      "### Euler's method\n",
      "\n",
      "To derive a numerical method, we can first turn once again to the Taylor\n",
      "series. In this case, we could write:\n",
      "\n",
      "$$ x(t+h)=x(t)+h x'(t) + O(h^2) $$\n",
      "\n",
      "Using the definition of our ODE above, we can substitute in for $x'(t)$:\n",
      "\n",
      "$$ x(t+h)=x(t)+h f(t,x(t))+ O(h^2)$$\n",
      "\n",
      "Notice that the value of $x$ used in the evaluation of $f$ is that at time $t$. This simple scheme is named **Euler's method** after the 18th century Swiss mathematician, Leonard Euler. \n",
      "\n",
      "The formula given is used to calculate the value of $x$ one time step forward from the last known value. The error is therefore the local truncation error. If we actually wish to know the value at some fixed time $T$ then we will have to calculate $(T-t_0)/h$ steps of the method. This sum over $O(1/h)$ steps results in a global truncation error for Euler's method of $O(h)$. In other words, Euler's method is first order.\n",
      "\n",
      "To illustrate Euler's method, and convey the fundamental idea of all time stepping methods, we'll use Euler's method to solve one of the simplest of all ODEs:\n",
      "\n",
      "$$ x'(t)=x $$\n",
      "$$ x(0)=1 $$\n",
      "\n",
      "We know, of course, that the solution to this equation is $x=e^t$, but let's ignore that for one moment and evaluate $x(0.1)$ using Euler's method with steps of $0.05$. The first step is:\n",
      "\n",
      "$$\\begin{align}\n",
      "  x(0.05)&\\approx x(0)+0.05x'(0)\\\\\n",
      "  &\\approx1+.05\\times1\\\\\n",
      "  &\\approx 1.05\n",
      "\\end{align}$$\n",
      "\n",
      "Now that we know $x(0.05)$, we can calculate the second step:\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "  x(0.1)&\\approx x(0.05)+0.05x(0.05)\\\\\n",
      "  &\\approx 1.05+.05\\times1.05\\\\\n",
      "  &\\approx 1.1025\n",
      "\\end{align}$$\n",
      "\n",
      "Now the actual value of $e^{0.1}$ is around $1.1051$ so we're a couple of percent off even over a very short time interval.\n",
      "\n",
      "### Heun's method\n",
      "\n",
      "Euler's method is first order because it calculates the derivative using only the information available at the beginning of the time step. As we observed previously, igher order convergence can be obtained if we also employ information from other points in the interval. The method known as Heun's method may be derived by attempting to use derivative information at both the start and the end of the interval:\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "  x(t+h)&\\approx x(t)+\\frac{h}{2}\\left(x'(t)+x'(t+h)\\right)\\\\\n",
      "  &\\approx x(t)+\\frac{h}{2}\\big(f(x,t)+f(t+h,x(t+h))\\big)\n",
      "\\end{align}$$\n",
      "\n",
      "The difficulty with this approach is that we now require $x(t+h)$ in order to calculate the final term in the equation, and that's what we set out to calculate so we don't know it yet! The solution to this dilemma adopted in Heun's method is to use a first guess at $x(t+h)$ calculated using Euler's method:\n",
      "\n",
      "$$ \\tilde{x}(t+h)=x(t)+hf(x,t) $$\n",
      "\n",
      "This first guess is then used to solve for $x(t+h)$ using:\n",
      "\n",
      "$$ x(t+h)\\approx x(t)+\\frac{h}{2}\\big(f(x,t)+f(t+h,\\tilde{x}(t+h))\\big)$$\n",
      "\n",
      "The generic term for schemes of this type is **predictor-corrector** The initial calculation of $\\tilde{x}(t+h)$ is used to predict the new value of $x$ and then this is used in a more accurate calculation to produce a more correct value. We will state without derivation or proof that the global truncation error in Heun's method is $O(h^2)$. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}